{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Mountain Goats Album Era with Sentiment Analysis\n",
    "This is a binary classification problem. I am predicting album era based on the sentiment intensity of the lyrics. In order to make era binary, I have separated the data according to the lo-fi years and the hi-fi years. This is a common conceptual split for fans. Lo-fi refers to low fidelity recordings, or recordings that are not faithful to the original sound due to background noise, static, etc. Hi-fi refers to high fidelity records, which are more polished sounding and more true to the original sound. \n",
    "\n",
    "I gathered lyrics from Kyle Barbour's [Annotated Mountain Goats site](https://annotatedtmg.org/) by album. I copied/pasted the lyrics for each album into a .txt file. Cleaning was partially manual and partially computational. I cleaned each file by manually removing song titles before using Python scripts to remove numbers. Herein, these .txt files are further cleaned, have stopwords removed, and are tokenized, both by word and by sentence. I tokenized using [NLTK](https://www.nltk.org/index.html).\n",
    "\n",
    "Sentiment polarity was calculated both using the tokenized words and sentences. I used NLTK's [VADER](https://www.nltk.org/_modules/nltk/sentiment/vader.html) to analyze sentiment intensity. I calculated mean, median, and standard deviation for the sentiment intensity scores for each album. I did this separately for sentence-tokenized and word-tokenized data. This gave me a lot of data features, so I also used the K best algorithm for feature selection with sklearn.\n",
    "\n",
    "I used four different classification algorithms on both the sentence- and word-tokenized data. The four classification algorithms I tried are Naive Bayes, K Nearest Neighbors, Decision Tree, and Random Forest. I did this using the whole set of numeric features and the k best features for each tokenized dataset. In both cases, feature selection via k best improved the model's accuracy and variability. Overall, the Random Forest classifier performed the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step One: Imports, Functions, and Variables. Oh my!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLTK\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emma/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# sklearn libraries\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Misc.\n",
    "import statistics as stat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global variables\n",
    "nums = string.digits\n",
    "punc = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get tokenized sentences and words\n",
    "def tokens(filename):\n",
    "    tmp_str = ''\n",
    "    \n",
    "    # Open file\n",
    "    with open(filename) as f:\n",
    "        f_in = f.read()\n",
    "    f.close()\n",
    "    \n",
    "    # Check for numbers or punctuation\n",
    "    # If a character is a number or punctuation mark,\n",
    "    # skip it and do not add it to the growing tmp string\n",
    "    for line in f_in:\n",
    "        line_str = str(line)\n",
    "        if line_str in nums or line_str in punc:\n",
    "            pass\n",
    "        else:\n",
    "            tmp_str += line_str\n",
    "            \n",
    "    # Replace newlines with a period + space to make it a sentence\n",
    "    # This is to help define sentences for tokenizing later\n",
    "    # Consolidate extraneous spaces\n",
    "    f_prep = tmp_str.replace('\\n','. ').replace('  ','')\n",
    "\n",
    "    # Tokenize sentences\n",
    "    # Some sentences became only \".\", so I removed those\n",
    "    f_sent = sent_tokenize(f_prep)\n",
    "    f_sent = [s for s in f_sent if s != '.']\n",
    "    \n",
    "    # Tokenize words\n",
    "    # Some words were stored as only \".\", so I removed those\n",
    "    f_words = word_tokenize(f_prep)\n",
    "    f_words = [s for s in f_words if s != '.']\n",
    "    \n",
    "    # Remove stop words\n",
    "    f_sent2 = [f for f in f_sent if f not in stop_words]\n",
    "    f_words2 = [f for f in f_words if f not in stop_words]\n",
    "    \n",
    "    # return the tokenized sentences and words\n",
    "    return f_sent2, f_words2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All text files are in their own directory (text)\n",
    "# Get sentences and individual words for each album\n",
    "aed_sentence, aed_words = tokens('text/all-eternals-deck.txt')\n",
    "btc_sentence, btc_words = tokens('text/beat-the-champ.txt')\n",
    "heretic_sentence, heretic_words = tokens('text/heretic-pride.txt')\n",
    "gl_sentence, gl_words = tokens('text/get-lonely.txt')\n",
    "life_sentence, life_words = tokens('text/tlotwtc.txt')\n",
    "ty_sentence, ty_words = tokens('text/transcendental-youth.txt')\n",
    "sunset_sentence, sunset_words = tokens('text/sunset-tree.txt')\n",
    "wsabh_sentence, wsabh_words = tokens('text/wsabh.txt')\n",
    "talla_sentence, talla_words = tokens('text/tallahassee.txt')\n",
    "ahwt_sentence, ahwt_words = tokens('text/ahwt.txt')\n",
    "tcg_sentence, tcg_words = tokens('text/the-coroners-gambit.txt')\n",
    "ffg_sentence, ffg_words = tokens('text/full-force-galesburg.txt')\n",
    "nfj_sentence, nfj_words = tokens('text/nothing-for-juice.txt')\n",
    "sweden_sentence, sweden_words = tokens('text/sweden.txt')\n",
    "zopilote_sentence, zopilote_words = tokens('text/zopilote-machine.txt')\n",
    "nfj_sentence, nfj_words = tokens('text/nothing-for-juice.txt')\n",
    "sweden_sentence, sweden_words = tokens('text/sweden.txt')\n",
    "zopilote_sentence, zopilote_words = tokens('text/zopilote-machine.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sentences\n",
    "album_lines = [\n",
    "    btc_sentence,\n",
    "          ty_sentence,\n",
    "          aed_sentence,\n",
    "          life_sentence,\n",
    "          heretic_sentence,\n",
    "          gl_sentence,\n",
    "          sunset_sentence,\n",
    "         wsabh_sentence,\n",
    "         talla_sentence,\n",
    "          ahwt_sentence,\n",
    "          tcg_sentence,\n",
    "          ffg_sentence,\n",
    "          nfj_sentence,\n",
    "          sweden_sentence,\n",
    "          zopilote_sentence\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Words\n",
    "album_words = [aed_words,\n",
    "          btc_words,\n",
    "          heretic_words,\n",
    "          gl_words,\n",
    "          life_words,\n",
    "          ty_words,\n",
    "          sunset_words,\n",
    "         wsabh_words,\n",
    "         talla_words,\n",
    "          ahwt_words,\n",
    "          tcg_words,\n",
    "          ffg_words,\n",
    "          nfj_words,\n",
    "          sweden_words,\n",
    "          zopilote_words    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Album names\n",
    "album_names = [\n",
    "    'Beat the Champ',\n",
    "    'Transcendental Youth',\n",
    "    'All Eternals Deck',\n",
    "    'The Life of the World to Come',\n",
    "    'Heretic Pride',\n",
    "    'Get Lonely',\n",
    "    'The Sunset Tree',\n",
    "    'We Shall All Be Healed',\n",
    "    'Tallahassee',\n",
    "    'All Hail West Texas',\n",
    "    \"The Coroner's Gambit\",\n",
    "    'Full Force Galesburg',\n",
    "    'Nothing for Juice',\n",
    "    'Sweden',\n",
    "    'Zopilote Machine'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Album release years\n",
    "album_years = [2015,\n",
    "          2012,\n",
    "          2011,\n",
    "          2009,\n",
    "          2008,\n",
    "          2006,\n",
    "          2005,\n",
    "          2004,\n",
    "          2002,\n",
    "          2002,\n",
    "          2000,\n",
    "          1997,\n",
    "            1996,\n",
    "            1995,\n",
    "            1994\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Album labels\n",
    "album_labels = ['Merge',\n",
    "             'Merge',\n",
    "             'Merge',\n",
    "             '4AD',\n",
    "             '4AD',\n",
    "             '4AD',\n",
    "             '4AD',\n",
    "             '4AD',\n",
    "             '4AD',\n",
    "             'Emperor Jones',\n",
    "             'Absolutely Kosher',\n",
    "             'Emperor Jones',\n",
    "             'Ajax',\n",
    "             'Shrimper',\n",
    "             'Ajax'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hi-fi or lo-fi \n",
    "fi = [\n",
    "    'hi-fi',\n",
    "    'hi-fi',\n",
    "    'hi-fi',\n",
    "    'hi-fi',\n",
    "    'hi-fi',\n",
    "    'hi-fi',\n",
    "    'hi-fi',\n",
    "    'hi-fi',\n",
    "    'hi-fi',\n",
    "    'lo-fi',\n",
    "    'lo-fi',\n",
    "    'lo-fi',\n",
    "    'lo-fi',\n",
    "    'lo-fi',\n",
    "    'lo-fi'\n",
    "]\n",
    "# Hi-fi or lo-fi represented as 1 or 0, respectively\n",
    "fi_binary = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lambda functions to get the negative, positive, \n",
    "# and neutral sentiment polarity scores individually\n",
    "neg = lambda n : [n[i]['neg'] for i in range(len(n))]\n",
    "posi = lambda p : [p[i]['pos'] for i in range(len(p))]\n",
    "neu = lambda ne : [ne[i]['neu'] for i in range(len(ne))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to get mean, median, and standard deviation of input\n",
    "def get_stats(info):\n",
    "    me = stat.mean(info)\n",
    "    med = stat.median(info)\n",
    "    sdev = stat.stdev(info)\n",
    "    return me,med,sdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to get sentiment polarities and statistics\n",
    "def get_sentiment(lyrics):\n",
    "    # Empty list to store each set of stats\n",
    "    stats_list = []\n",
    "    \n",
    "    # Run the sentiment intensity analysis for input parameter\n",
    "    for l in lyrics:\n",
    "        # Get polarity scores\n",
    "        tmp_sia = [sia.polarity_scores(y) for y in l]\n",
    "        # Get mean, median, and standard dev\n",
    "        # For negative, positive, and neutral sentiments\n",
    "        tmp_neg_one,tmp_neg_two,tmp_neg_three = get_stats(neg(tmp_sia))\n",
    "        tmp_pos_one,tmp_pos_two,tmp_pos_three = get_stats(posi(tmp_sia))\n",
    "        tmp_neu_one,tmp_neu_two,tmp_neu_three = get_stats(neu(tmp_sia))\n",
    "        \n",
    "        # Add all stats to temporary list\n",
    "        tmp_list = [tmp_neg_one,tmp_neg_two,tmp_neg_three,\n",
    "                    tmp_pos_one,tmp_pos_two,tmp_pos_three,\n",
    "                    tmp_neu_one,tmp_neu_two,tmp_neu_three\n",
    "                   ]\n",
    "        # Make stats_list a list of lists\n",
    "        stats_list.append(tmp_list)\n",
    "        \n",
    "    # Return the list of lists    \n",
    "    return stats_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Statistics for sentences\n",
    "album_stats_lines = get_sentiment(album_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Statistics for words\n",
    "album_stats_words = get_sentiment(album_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to make pandas data frames\n",
    "# Since this is the only dataset in use, the\n",
    "# column names are hard-coded in to make it easy\n",
    "def make_df(stats_list):\n",
    "    album_list = []\n",
    "    for i in range(15):\n",
    "        tmp = [album_names[i],album_labels[i],fi[i],album_years[i]]\n",
    "        tmp_list = tmp + stats_list[i]\n",
    "        album_list.append(tmp_list)\n",
    "    my_df = pd.DataFrame(data = album_list,\n",
    "                         columns = ['Album.Name',\n",
    "                                    'Album.Label',\n",
    "                                    'Hi.Lo.Fi',\n",
    "                                    'Album.Year',\n",
    "                                    'Avg.Neg','Med.Neg','StDev.Neg',\n",
    "             'Avg.Posi','Med.Posi','StDev.Posi',\n",
    "            'Avg.Neu','Med.Neu','StDev.Neu']\n",
    "                        )\n",
    "    return my_df\n",
    "                                    \n",
    "                         \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Two: Visualize with Pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first shown DataFrame contains statistics about the sentiment of the lines in each song on an album. These were generated with nltk's sentence tokenizer. The second DataFrame contains the same statistics, but calculated based on the sentiment scores of individual words. This was done with nltk's word tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Album.Name</th>\n",
       "      <th>Album.Label</th>\n",
       "      <th>Hi.Lo.Fi</th>\n",
       "      <th>Album.Year</th>\n",
       "      <th>Avg.Neg</th>\n",
       "      <th>Med.Neg</th>\n",
       "      <th>StDev.Neg</th>\n",
       "      <th>Avg.Posi</th>\n",
       "      <th>Med.Posi</th>\n",
       "      <th>StDev.Posi</th>\n",
       "      <th>Avg.Neu</th>\n",
       "      <th>Med.Neu</th>\n",
       "      <th>StDev.Neu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beat the Champ</td>\n",
       "      <td>Merge</td>\n",
       "      <td>hi-fi</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.097624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.201724</td>\n",
       "      <td>0.103033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181572</td>\n",
       "      <td>0.799352</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transcendental Youth</td>\n",
       "      <td>Merge</td>\n",
       "      <td>hi-fi</td>\n",
       "      <td>2012</td>\n",
       "      <td>0.073420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149858</td>\n",
       "      <td>0.106268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.188400</td>\n",
       "      <td>0.820313</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.221928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All Eternals Deck</td>\n",
       "      <td>Merge</td>\n",
       "      <td>hi-fi</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.057328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137868</td>\n",
       "      <td>0.107996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.195333</td>\n",
       "      <td>0.834683</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.231174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Life of the World to Come</td>\n",
       "      <td>4AD</td>\n",
       "      <td>hi-fi</td>\n",
       "      <td>2009</td>\n",
       "      <td>0.068580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149791</td>\n",
       "      <td>0.069929</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.146388</td>\n",
       "      <td>0.861494</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Heretic Pride</td>\n",
       "      <td>4AD</td>\n",
       "      <td>hi-fi</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.067439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149653</td>\n",
       "      <td>0.086585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.194525</td>\n",
       "      <td>0.845980</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.228919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Get Lonely</td>\n",
       "      <td>4AD</td>\n",
       "      <td>hi-fi</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.067858</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.160274</td>\n",
       "      <td>0.062006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133065</td>\n",
       "      <td>0.870140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.195595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Sunset Tree</td>\n",
       "      <td>4AD</td>\n",
       "      <td>hi-fi</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.064758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.140908</td>\n",
       "      <td>0.071815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155714</td>\n",
       "      <td>0.863433</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.202443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We Shall All Be Healed</td>\n",
       "      <td>4AD</td>\n",
       "      <td>hi-fi</td>\n",
       "      <td>2004</td>\n",
       "      <td>0.042608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.118405</td>\n",
       "      <td>0.085246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.164869</td>\n",
       "      <td>0.872147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.202518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tallahassee</td>\n",
       "      <td>4AD</td>\n",
       "      <td>hi-fi</td>\n",
       "      <td>2002</td>\n",
       "      <td>0.072182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150827</td>\n",
       "      <td>0.109621</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197108</td>\n",
       "      <td>0.818205</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.249920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>All Hail West Texas</td>\n",
       "      <td>Emperor Jones</td>\n",
       "      <td>lo-fi</td>\n",
       "      <td>2002</td>\n",
       "      <td>0.060354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137448</td>\n",
       "      <td>0.096566</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181431</td>\n",
       "      <td>0.843078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.218602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The Coroner's Gambit</td>\n",
       "      <td>Absolutely Kosher</td>\n",
       "      <td>lo-fi</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.056848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.134119</td>\n",
       "      <td>0.092982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.190851</td>\n",
       "      <td>0.850176</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.223955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Full Force Galesburg</td>\n",
       "      <td>Emperor Jones</td>\n",
       "      <td>lo-fi</td>\n",
       "      <td>1997</td>\n",
       "      <td>0.041686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115350</td>\n",
       "      <td>0.069101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155168</td>\n",
       "      <td>0.889213</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.185662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Nothing for Juice</td>\n",
       "      <td>Ajax</td>\n",
       "      <td>lo-fi</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.053184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.167142</td>\n",
       "      <td>0.062068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.154257</td>\n",
       "      <td>0.884735</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.226578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Shrimper</td>\n",
       "      <td>lo-fi</td>\n",
       "      <td>1995</td>\n",
       "      <td>0.063230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.146595</td>\n",
       "      <td>0.084877</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185978</td>\n",
       "      <td>0.851896</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.243337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Zopilote Machine</td>\n",
       "      <td>Ajax</td>\n",
       "      <td>lo-fi</td>\n",
       "      <td>1994</td>\n",
       "      <td>0.060284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155744</td>\n",
       "      <td>0.088086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.205727</td>\n",
       "      <td>0.851633</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.245204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Album.Name        Album.Label Hi.Lo.Fi  Album.Year  \\\n",
       "0                  Beat the Champ              Merge    hi-fi        2015   \n",
       "1            Transcendental Youth              Merge    hi-fi        2012   \n",
       "2               All Eternals Deck              Merge    hi-fi        2011   \n",
       "3   The Life of the World to Come                4AD    hi-fi        2009   \n",
       "4                   Heretic Pride                4AD    hi-fi        2008   \n",
       "5                      Get Lonely                4AD    hi-fi        2006   \n",
       "6                 The Sunset Tree                4AD    hi-fi        2005   \n",
       "7          We Shall All Be Healed                4AD    hi-fi        2004   \n",
       "8                     Tallahassee                4AD    hi-fi        2002   \n",
       "9             All Hail West Texas      Emperor Jones    lo-fi        2002   \n",
       "10           The Coroner's Gambit  Absolutely Kosher    lo-fi        2000   \n",
       "11           Full Force Galesburg      Emperor Jones    lo-fi        1997   \n",
       "12              Nothing for Juice               Ajax    lo-fi        1996   \n",
       "13                         Sweden           Shrimper    lo-fi        1995   \n",
       "14               Zopilote Machine               Ajax    lo-fi        1994   \n",
       "\n",
       "     Avg.Neg  Med.Neg  StDev.Neg  Avg.Posi  Med.Posi  StDev.Posi   Avg.Neu  \\\n",
       "0   0.097624      0.0   0.201724  0.103033       0.0    0.181572  0.799352   \n",
       "1   0.073420      0.0   0.149858  0.106268       0.0    0.188400  0.820313   \n",
       "2   0.057328      0.0   0.137868  0.107996       0.0    0.195333  0.834683   \n",
       "3   0.068580      0.0   0.149791  0.069929       0.0    0.146388  0.861494   \n",
       "4   0.067439      0.0   0.149653  0.086585       0.0    0.194525  0.845980   \n",
       "5   0.067858      0.0   0.160274  0.062006       0.0    0.133065  0.870140   \n",
       "6   0.064758      0.0   0.140908  0.071815       0.0    0.155714  0.863433   \n",
       "7   0.042608      0.0   0.118405  0.085246       0.0    0.164869  0.872147   \n",
       "8   0.072182      0.0   0.150827  0.109621       0.0    0.197108  0.818205   \n",
       "9   0.060354      0.0   0.137448  0.096566       0.0    0.181431  0.843078   \n",
       "10  0.056848      0.0   0.134119  0.092982       0.0    0.190851  0.850176   \n",
       "11  0.041686      0.0   0.115350  0.069101       0.0    0.155168  0.889213   \n",
       "12  0.053184      0.0   0.167142  0.062068       0.0    0.154257  0.884735   \n",
       "13  0.063230      0.0   0.146595  0.084877       0.0    0.185978  0.851896   \n",
       "14  0.060284      0.0   0.155744  0.088086       0.0    0.205727  0.851633   \n",
       "\n",
       "    Med.Neu  StDev.Neu  \n",
       "0       1.0   0.250419  \n",
       "1       1.0   0.221928  \n",
       "2       1.0   0.231174  \n",
       "3       1.0   0.200583  \n",
       "4       1.0   0.228919  \n",
       "5       1.0   0.195595  \n",
       "6       1.0   0.202443  \n",
       "7       1.0   0.202518  \n",
       "8       1.0   0.249920  \n",
       "9       1.0   0.218602  \n",
       "10      1.0   0.223955  \n",
       "11      1.0   0.185662  \n",
       "12      1.0   0.226578  \n",
       "13      1.0   0.243337  \n",
       "14      1.0   0.245204  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make and display dataframe for sentences statistics\n",
    "goats_df = make_df(album_stats_lines)\n",
    "goats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Album.Name</th>\n",
       "      <th>Album.Label</th>\n",
       "      <th>Hi.Lo.Fi</th>\n",
       "      <th>Album.Year</th>\n",
       "      <th>Avg.Neg</th>\n",
       "      <th>Med.Neg</th>\n",
       "      <th>StDev.Neg</th>\n",
       "      <th>Avg.Posi</th>\n",
       "      <th>Med.Posi</th>\n",
       "      <th>StDev.Posi</th>\n",
       "      <th>Avg.Neu</th>\n",
       "      <th>Med.Neu</th>\n",
       "      <th>StDev.Neu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beat the Champ</td>\n",
       "      <td>Merge</td>\n",
       "      <td>hi-fi</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.044470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.206195</td>\n",
       "      <td>0.080388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.271970</td>\n",
       "      <td>0.855758</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.351435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transcendental Youth</td>\n",
       "      <td>Merge</td>\n",
       "      <td>hi-fi</td>\n",
       "      <td>2012</td>\n",
       "      <td>0.068724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253072</td>\n",
       "      <td>0.082048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.274534</td>\n",
       "      <td>0.819074</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.385092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All Eternals Deck</td>\n",
       "      <td>Merge</td>\n",
       "      <td>hi-fi</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.051188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220449</td>\n",
       "      <td>0.063376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.243712</td>\n",
       "      <td>0.834857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.371423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Life of the World to Come</td>\n",
       "      <td>4AD</td>\n",
       "      <td>hi-fi</td>\n",
       "      <td>2009</td>\n",
       "      <td>0.048679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.215270</td>\n",
       "      <td>0.052156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222418</td>\n",
       "      <td>0.819889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.384414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Heretic Pride</td>\n",
       "      <td>4AD</td>\n",
       "      <td>hi-fi</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.050294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.218622</td>\n",
       "      <td>0.063357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.243684</td>\n",
       "      <td>0.826911</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.378448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Get Lonely</td>\n",
       "      <td>4AD</td>\n",
       "      <td>hi-fi</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.059303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.236279</td>\n",
       "      <td>0.086731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.281545</td>\n",
       "      <td>0.828021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.377502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Sunset Tree</td>\n",
       "      <td>4AD</td>\n",
       "      <td>hi-fi</td>\n",
       "      <td>2005</td>\n",
       "      <td>0.049574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.217147</td>\n",
       "      <td>0.055771</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.229567</td>\n",
       "      <td>0.848954</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.358232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We Shall All Be Healed</td>\n",
       "      <td>4AD</td>\n",
       "      <td>hi-fi</td>\n",
       "      <td>2004</td>\n",
       "      <td>0.036558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.187744</td>\n",
       "      <td>0.074638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.262907</td>\n",
       "      <td>0.843869</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.363118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tallahassee</td>\n",
       "      <td>4AD</td>\n",
       "      <td>hi-fi</td>\n",
       "      <td>2002</td>\n",
       "      <td>0.058533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.234832</td>\n",
       "      <td>0.092384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.289669</td>\n",
       "      <td>0.797602</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.401929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>All Hail West Texas</td>\n",
       "      <td>Emperor Jones</td>\n",
       "      <td>lo-fi</td>\n",
       "      <td>2002</td>\n",
       "      <td>0.046989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.211664</td>\n",
       "      <td>0.073449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.260931</td>\n",
       "      <td>0.812044</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.390766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The Coroner's Gambit</td>\n",
       "      <td>Absolutely Kosher</td>\n",
       "      <td>lo-fi</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.044397</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.206049</td>\n",
       "      <td>0.068358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.252448</td>\n",
       "      <td>0.823820</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.381108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Full Force Galesburg</td>\n",
       "      <td>Emperor Jones</td>\n",
       "      <td>lo-fi</td>\n",
       "      <td>1997</td>\n",
       "      <td>0.032860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.178338</td>\n",
       "      <td>0.054518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.227122</td>\n",
       "      <td>0.849141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.358045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Nothing for Juice</td>\n",
       "      <td>Ajax</td>\n",
       "      <td>lo-fi</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.037221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.189381</td>\n",
       "      <td>0.046319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.210263</td>\n",
       "      <td>0.832093</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.373939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Shrimper</td>\n",
       "      <td>lo-fi</td>\n",
       "      <td>1995</td>\n",
       "      <td>0.040584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197406</td>\n",
       "      <td>0.064123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.245072</td>\n",
       "      <td>0.811688</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.391120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Zopilote Machine</td>\n",
       "      <td>Ajax</td>\n",
       "      <td>lo-fi</td>\n",
       "      <td>1994</td>\n",
       "      <td>0.037451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.189926</td>\n",
       "      <td>0.053876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.225848</td>\n",
       "      <td>0.838371</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.368232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Album.Name        Album.Label Hi.Lo.Fi  Album.Year  \\\n",
       "0                  Beat the Champ              Merge    hi-fi        2015   \n",
       "1            Transcendental Youth              Merge    hi-fi        2012   \n",
       "2               All Eternals Deck              Merge    hi-fi        2011   \n",
       "3   The Life of the World to Come                4AD    hi-fi        2009   \n",
       "4                   Heretic Pride                4AD    hi-fi        2008   \n",
       "5                      Get Lonely                4AD    hi-fi        2006   \n",
       "6                 The Sunset Tree                4AD    hi-fi        2005   \n",
       "7          We Shall All Be Healed                4AD    hi-fi        2004   \n",
       "8                     Tallahassee                4AD    hi-fi        2002   \n",
       "9             All Hail West Texas      Emperor Jones    lo-fi        2002   \n",
       "10           The Coroner's Gambit  Absolutely Kosher    lo-fi        2000   \n",
       "11           Full Force Galesburg      Emperor Jones    lo-fi        1997   \n",
       "12              Nothing for Juice               Ajax    lo-fi        1996   \n",
       "13                         Sweden           Shrimper    lo-fi        1995   \n",
       "14               Zopilote Machine               Ajax    lo-fi        1994   \n",
       "\n",
       "     Avg.Neg  Med.Neg  StDev.Neg  Avg.Posi  Med.Posi  StDev.Posi   Avg.Neu  \\\n",
       "0   0.044470      0.0   0.206195  0.080388       0.0    0.271970  0.855758   \n",
       "1   0.068724      0.0   0.253072  0.082048       0.0    0.274534  0.819074   \n",
       "2   0.051188      0.0   0.220449  0.063376       0.0    0.243712  0.834857   \n",
       "3   0.048679      0.0   0.215270  0.052156       0.0    0.222418  0.819889   \n",
       "4   0.050294      0.0   0.218622  0.063357       0.0    0.243684  0.826911   \n",
       "5   0.059303      0.0   0.236279  0.086731       0.0    0.281545  0.828021   \n",
       "6   0.049574      0.0   0.217147  0.055771       0.0    0.229567  0.848954   \n",
       "7   0.036558      0.0   0.187744  0.074638       0.0    0.262907  0.843869   \n",
       "8   0.058533      0.0   0.234832  0.092384       0.0    0.289669  0.797602   \n",
       "9   0.046989      0.0   0.211664  0.073449       0.0    0.260931  0.812044   \n",
       "10  0.044397      0.0   0.206049  0.068358       0.0    0.252448  0.823820   \n",
       "11  0.032860      0.0   0.178338  0.054518       0.0    0.227122  0.849141   \n",
       "12  0.037221      0.0   0.189381  0.046319       0.0    0.210263  0.832093   \n",
       "13  0.040584      0.0   0.197406  0.064123       0.0    0.245072  0.811688   \n",
       "14  0.037451      0.0   0.189926  0.053876       0.0    0.225848  0.838371   \n",
       "\n",
       "    Med.Neu  StDev.Neu  \n",
       "0       1.0   0.351435  \n",
       "1       1.0   0.385092  \n",
       "2       1.0   0.371423  \n",
       "3       1.0   0.384414  \n",
       "4       1.0   0.378448  \n",
       "5       1.0   0.377502  \n",
       "6       1.0   0.358232  \n",
       "7       1.0   0.363118  \n",
       "8       1.0   0.401929  \n",
       "9       1.0   0.390766  \n",
       "10      1.0   0.381108  \n",
       "11      1.0   0.358045  \n",
       "12      1.0   0.373939  \n",
       "13      1.0   0.391120  \n",
       "14      1.0   0.368232  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make and display dataframe for word statistics\n",
    "goats_df2 = make_df(album_stats_words)\n",
    "goats_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Three: How well can album sentiment predict album era?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I made each DataFrame into a numpy array. Next, I removed the non-numeric data. I also removed the year. As stated previously, lo-fi and hi-fi albums correspond to certain years, hence using the binary distinction of lo- or hi-fi for era. I did this first for the sentence-tokenized and second for the word-tokenized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a binary classification problem on a small dataset. I will be trying multiple classification algorithms from sklearn and performing 5-fold cross validation. The classifiers I will are trying are:\n",
    "* Naive Bayes\n",
    "* K Nearest Neighbors\n",
    "* Decision Tree\n",
    "* Random Forest\n",
    "\n",
    "I created a function (below) to generate a DataFrame with information about the performance of each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_clf_df(input_array):\n",
    "    clf_nb = GaussianNB()\n",
    "    clf_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    clf_dt = DecisionTreeClassifier(random_state=0)\n",
    "    clf_rf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "    \n",
    "    nb_scores = cross_val_score(clf_nb, input_array, fi_binary, cv=5)\n",
    "    knn_scores = cross_val_score(clf_knn, input_array, fi_binary, cv=5)\n",
    "    dt_scores = cross_val_score(clf_dt, input_array, fi_binary, cv=5)\n",
    "    rf_scores = cross_val_score(clf_rf, input_array, fi_binary, cv=5)\n",
    "    \n",
    "    scores_list = [nb_scores,knn_scores,dt_scores,rf_scores]\n",
    "    \n",
    "    df_data = [(s.mean(),s.std()) for s in scores_list]\n",
    "    \n",
    "    my_df = pd.DataFrame(data = df_data,\n",
    "                         index = ['Naive Bayes',\n",
    "                                    'K Nearest Neighbors',\n",
    "                                    'Decision Tree',\n",
    "                                    'Random Forest'],\n",
    "                         columns = ['Mean',\n",
    "                                 'Standard.Dev']\n",
    "                        )\n",
    "    return my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence-Tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goats_np = goats_df.to_numpy()\n",
    "goats_nums = [g[4:14] for g in goats_np]\n",
    "goats_nums_df = pd.DataFrame(data=goats_nums)\n",
    "goats_np_num = goats_nums_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Standard.Dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.081650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K Nearest Neighbors</th>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.249444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.226078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.152753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Mean  Standard.Dev\n",
       "Naive Bayes          0.400000      0.081650\n",
       "K Nearest Neighbors  0.566667      0.249444\n",
       "Decision Tree        0.600000      0.226078\n",
       "Random Forest        0.816667      0.152753"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_df = make_clf_df(goats_np_num)\n",
    "whole_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest classifier is outperforming the others by a significant margin. While the mean Random Forest accuracy score is ~82, the accuracy varied substantially given the high standard deviation. Still, the RF standard deviation is lower than both the Decision Tree and KNN approaches. Naive Bayes had the lowest variability, but is also the least accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of features in this data set, so I decided to use the k best algorithm for feature selection. Using k=4, the highest scoring features are the mean and standard deviation for negative sentiment intensity score, the mean positive sentiment intensity score, and the mean neutral sentiment intensity score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emma/anaconda3/lib/python3.6/site-packages/sklearn/utils/__init__.py:54: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.09762388, 0.20172438, 0.10303284, 0.79935224],\n",
       "       [0.07341972, 0.14985775, 0.10626761, 0.82031268],\n",
       "       [0.05732766, 0.13786792, 0.10799574, 0.83468298],\n",
       "       [0.06857988, 0.14979107, 0.06992899, 0.86149408],\n",
       "       [0.06743946, 0.1496531 , 0.0865852 , 0.84597982],\n",
       "       [0.06785756, 0.16027391, 0.06200581, 0.87013953],\n",
       "       [0.06475839, 0.14090822, 0.07181544, 0.86343289],\n",
       "       [0.04260778, 0.11840521, 0.08524551, 0.87214671],\n",
       "       [0.07218159, 0.15082707, 0.10962148, 0.8182046 ],\n",
       "       [0.060354  , 0.13744771, 0.096566  , 0.843078  ],\n",
       "       [0.05684821, 0.13411923, 0.09298214, 0.8501756 ],\n",
       "       [0.04168598, 0.11535008, 0.06910061, 0.88921341],\n",
       "       [0.05318387, 0.16714171, 0.06206774, 0.88473548],\n",
       "       [0.06322977, 0.14659495, 0.08487702, 0.85189644],\n",
       "       [0.06028367, 0.1557444 , 0.08808596, 0.85163324]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = goats_np_num, fi_binary\n",
    "k_best = SelectKBest(chi2, k=4).fit_transform(x,y)\n",
    "k_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Standard.Dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.258199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K Nearest Neighbors</th>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.249444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.152753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Mean  Standard.Dev\n",
       "Naive Bayes          0.500000      0.258199\n",
       "K Nearest Neighbors  0.566667      0.249444\n",
       "Decision Tree        0.700000      0.266667\n",
       "Random Forest        0.816667      0.152753"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_best_df = make_clf_df(k_best)\n",
    "k_best_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the RF approach and the KNN approach are just as accurate and variable as before. However, the accuracy for the other two algorithms is much higher, although they still vary a lot. The Naive Bayes and Decision Tree approaches both became 10% more accurate, but the variability of NB rose substantially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word-Tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goats_np2 = goats_df2.to_numpy()\n",
    "goats_nums2 = [g[4:14] for g in goats_np2]\n",
    "goats_nums_df2 = pd.DataFrame(data=goats_nums2)\n",
    "goats_np_num2 = goats_nums_df2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Standard.Dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K Nearest Neighbors</th>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.145297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.152753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Mean  Standard.Dev\n",
       "Naive Bayes          0.700000      0.266667\n",
       "K Nearest Neighbors  0.766667      0.200000\n",
       "Decision Tree        0.883333      0.145297\n",
       "Random Forest        0.816667      0.152753"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_df2 = make_clf_df(goats_np_num2)\n",
    "whole_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the word-tokenized data, the Decision Tree approach is leading the pack in accuracy and has the lowest standard deviation. Random Forest is still doing quite well and its performance is equal to that of the sentence-tokenized data. The accuracy of NB and KNN are much higher, both now 20% more accurate. For both NB and KNN, the standard deviation has decreased, although it is still relatively high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also used k best with k=4 for feature selection in the word-tokenized data set. This time, the highest scoring features were different. They were mean and standard deviation for both negative sentiment intensity score and positive sentiment intensity score. Neutral scores were not as high scoring for the word-tokenized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emma/anaconda3/lib/python3.6/site-packages/sklearn/utils/__init__.py:54: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(mask.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.04446978, 0.2061952 , 0.08038769, 0.27196999],\n",
       "       [0.0687237 , 0.25307246, 0.08204769, 0.27453363],\n",
       "       [0.0511883 , 0.2204488 , 0.06337599, 0.24371227],\n",
       "       [0.04867872, 0.21527036, 0.05215577, 0.22241841],\n",
       "       [0.05029393, 0.2186222 , 0.06335728, 0.24368407],\n",
       "       [0.05930319, 0.23627887, 0.08673091, 0.2815447 ],\n",
       "       [0.04957397, 0.21714723, 0.05577072, 0.22956735],\n",
       "       [0.0365575 , 0.18774423, 0.07463823, 0.26290685],\n",
       "       [0.05853315, 0.23483165, 0.09238364, 0.28966891],\n",
       "       [0.04698905, 0.21166369, 0.07344891, 0.26093146],\n",
       "       [0.04439746, 0.20604914, 0.068358  , 0.2524482 ],\n",
       "       [0.03286034, 0.17833758, 0.0545183 , 0.22712238],\n",
       "       [0.03722084, 0.18938088, 0.04631927, 0.21026261],\n",
       "       [0.04058442, 0.19740555, 0.06412338, 0.24507207],\n",
       "       [0.03745072, 0.18992595, 0.05387648, 0.22584799]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2,y2 = goats_np_num2, fi_binary\n",
    "k_best2 = SelectKBest(chi2, k=4).fit_transform(x2,y2)\n",
    "k_best2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Standard.Dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K Nearest Neighbors</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.210819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.145297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.145297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Mean  Standard.Dev\n",
       "Naive Bayes          0.766667      0.200000\n",
       "K Nearest Neighbors  0.833333      0.210819\n",
       "Decision Tree        0.883333      0.145297\n",
       "Random Forest        0.883333      0.145297"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_best_df2 = make_clf_df(k_best2)\n",
    "k_best_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, KNN, DT, and RF are equally accurate and DT and RF have the same variability. NB is at its most accurate level with its lowest variability, but does not come close to the other algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
